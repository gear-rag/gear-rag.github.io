<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <meta name="description" content="GeAR: Graph-enhanced Agent for Retrieval-augmented Generation">
  <meta property="og:title" content="GeAR: Graph-enhanced Agent for Retrieval-augmented Generation"/>
  <meta property="og:description" content="A novel framework that incorporates a graph-based retriever within a multi-step retrieval agent for multi-hop question answering"/>
  <meta property="og:url" content="https://gear-rag.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/red-gear.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="GeAR: Graph-enhanced Agent for RAG">
  <meta name="twitter:description" content="A graph-enhanced framework for retrieval-augmented generation achieving SOTA on multi-hop QA">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/gears.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="retrieval augmented generation, RAG, multi-hop question answering, graph-based retrieval, large language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</title>
  <link rel="icon" type="image/png" href="static/images/red-gear.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GeAR: Graph-enhanced Agent for Retrieval-augmented Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=Fdlwl5QAAAAJ&hl=en" target="_blank">Zhili Shen</a><sup>†</sup>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=WAVUdOYAAAAJ&hl=en" target="_blank">Chenxin Diao</a><sup>†</sup>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=9J7YeR0AAAAJ&hl=en" target="_blank">Pavlos Vougiouklis</a><sup>†</sup>,</span>
                <span class="author-block">
                    <a href="https://pascualmeritatorres.github.io/" target="_blank">Pascual Merita</a><sup>†</sup>,</span><br>
                <span class="author-block">
                    <a href="https://arxiv.org/search/?searchtype=author&query=Shriram%20Piramanayagam" target="_blank">Shriram Piramanayagam</a>,</span>
                <span class="author-block">
                    <a href="https://dgraux.github.io/" target="_blank">Damien Graux</a>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=nf8bdFYAAAAJ&hl=zh-CN" target="_blank">Dandan Tu</a>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=Ji76oogAAAAJ&hl=zh-CN" target="_blank">Zeren Jiang</a>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=_V1aEUcAAAAJ&hl=zh-CN" target="_blank">Ruofei Lai</a>,</span>
                <span class="author-block">
                    <a href="https://scholar.google.ca/citations?user=BDTV6PQAAAAJ&hl=en" target="_blank">Yang Ren</a>,</span>
                <span class="author-block">
                    <a href="https://knowledge-representation.org/j.z.pan/" target="_blank">Jeff Z. Pan</a>,</span>
            </div>

            <div class="is-size-5 publication-authors">
                <span class="author-block"><span>Huawei Technologies Co., Ltd., Edinburgh, United Kingdom</span><br>Submitted to ACL 2025</span>
              <span class="eql-cntrb"><small><br><sup>†</sup>Equal Contribution</small></span>
            </div>


            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv  link -->
                <span class="link-block">
                    <a href="https://arxiv.org/abs/2412.18431" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv" style="color: #B31B1B;"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="static/pdfs/gear-pdf.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>PDF</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Retrieval-augmented generation systems rely on effective document retrieval capabilities. By design, conventional sparse or dense retrievers face challenges in multi-hop retrieval scenarios. In this paper, we present GeAR, which advances RAG performance through two key innovations: (i) graph expansion, which enhances any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates graph expansion. Our evaluation demonstrates GeAR's superior retrieval performance on three multi-hop question answering datasets. Additionally, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while requiring fewer tokens and iterations compared to other multi-step retrieval systems.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- How it Works section -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">How it Works</h2>
          <div class="content has-text-justified">
            <p>
              GeAR works by combining any retriever with a LLM agent to tackle complex multi-hop questions. The system first uses a base retriever (like BM25) to fetch initial passages, then employs a novel graph expansion technique called SyncGE to discover related content. During graph expansion, GeAR uses a language model to identify important information triples from the retrieved passages, then explores connections between these triples using diverse beam search. This helps bridge passages that may be several reasoning steps apart. The system maintains a "gist memory" that accumulates key information across multiple retrieval steps, similar to how the human brain's hippocampus processes memories. This approach allows GeAR to build comprehensive reasoning chains while using fewer computational resources than previous methods. The result is a more efficient system that achieves state-of-the-art performance on challenging multi-hop question answering tasks.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!--BibTeX citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{shen2024gear,
  title={GeAR: Graph-enhanced Agent for Retrieval-augmented Generation},
  author={Shen, Zhili and Diao, Chenxin and Vougiouklis, Pavlos and Merita, Pascual and Piramanayagam, Shriram and Graux, Damien and Tu, Dandan and Jiang, Zeren and Lai, Ruofei and Ren, Yang and Pan, Jeff Z.},
  journal={arXiv preprint arXiv:2412.18431},
  year={2024}
}</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
              Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>
</html>